{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04053c1-17da-480e-9d65-f8d9e42241fc",
   "metadata": {},
   "source": [
    "***TITLE: FN-NET LIGHTWEIGHT CNN MODEL FOR FABRIC DEFECT DETECTION***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1df78b-b773-428e-932b-e71425e22b16",
   "metadata": {},
   "source": [
    "***IMPORT LIBRARIES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1a02f2-0f01-480d-900e-2f4bbb39d745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 16:44:48.650311: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-04 16:44:48.659411: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746377088.670715    1869 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746377088.674785    1869 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-04 16:44:48.685836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import StratifiedKFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7b5647-eb74-43b7-a35b-859ce2f12f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d50764-d290-4e5d-8123-b6718fb525c6",
   "metadata": {},
   "source": [
    "***DATA PREPROCESSING***\n",
    "\n",
    "**1)** **DATASET IMPORTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc01e44-cec4-40d2-87b4-2fa6b0d42e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, img_size, num_classes, shuffle=True, class_indices=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.classes = np.array(labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.class_indices = class_indices or self._build_class_indices(labels)\n",
    "        self.on_epoch_end()\n",
    "        self.current_index=0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Initialize the iterator.\"\"\"\n",
    "        self.current_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Get the next batch.\"\"\"\n",
    "        if self.current_index >= len(self):\n",
    "            # End of epoch\n",
    "            self.on_epoch_end()\n",
    "            self.current_index = 0\n",
    "            raise StopIteration\n",
    "        \n",
    "        batch = self.__getitem__(self.current_index)\n",
    "        self.current_index += 1\n",
    "        return batch\n",
    "        \n",
    "    def _build_class_indices(self, labels):\n",
    "        unique_labels = sorted(set(labels))\n",
    "        return {str(lbl): lbl for lbl in unique_labels}\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_paths = self.image_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        batch_images = []\n",
    "        for path in batch_paths:\n",
    "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, self.img_size)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "\n",
    "            # Sobel edge detection\n",
    "            sobelx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)\n",
    "            sobely = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=3)\n",
    "\n",
    "            # Normalize edges to 0-1\n",
    "            sobelx = cv2.normalize(sobelx, None, 0, 1, cv2.NORM_MINMAX)\n",
    "            sobely = cv2.normalize(sobely, None, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "            #stacked = np.stack([img,  sobelx, sobely], axis=-1)\n",
    "            stacked =np.stack([img], axis=-1)\n",
    "            batch_images.append(stacked)\n",
    "\n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_labels = tf.keras.utils.to_categorical(batch_labels, self.num_classes)\n",
    "\n",
    "        return batch_images, batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            combined = list(zip(self.image_paths, self.labels))\n",
    "            np.random.shuffle(combined)\n",
    "            self.image_paths, self.labels = map(list, zip(*combined))  # Convert back to lists\n",
    "            self.classes = np.array(self.labels)  # Update classes array as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a57f297-7af3-4380-9d16-c7fc84406162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "train_dir = \"/mnt/c/newTrain/Train\"\n",
    "\n",
    "test_dir = \"/mnt/c/newTrain/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc38317-09ee-4795-8388-d40be9ff539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_filepaths_and_labels(directory):\n",
    "    class_names = sorted(os.listdir(directory))\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for fname in glob(os.path.join(class_dir, \"*\")):\n",
    "            filepaths.append(fname)\n",
    "            labels.append(class_name)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    class_indices = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    return filepaths, encoded_labels, len(label_encoder.classes_), label_encoder,class_indices\n",
    "# Image parameters\n",
    "IMG_SIZE = (256,256 )  # Image size\n",
    "BATCH_SIZE = 40       # Number of images in each batch\n",
    "# Train\n",
    "\n",
    "train_filepaths, train_labels, NUM_CLASSES, label_encoder,class_indices = get_filepaths_and_labels(train_dir)\n",
    "test_filepaths, test_labels, _, _,_ = get_filepaths_and_labels(test_dir)\n",
    "\n",
    "# Combine for full dataset cross-validation\n",
    "all_filepaths = np.array(train_filepaths + test_filepaths)\n",
    "all_labels = np.array(train_labels.tolist() + test_labels.tolist())\n",
    "\n",
    "train_generator = CustomDataGenerator(train_filepaths, train_labels, BATCH_SIZE, IMG_SIZE, NUM_CLASSES,class_indices=class_indices)\n",
    "test_generator = CustomDataGenerator(test_filepaths, test_labels, BATCH_SIZE, IMG_SIZE, NUM_CLASSES, shuffle=False,class_indices=class_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e5fba4-470a-4421-94b1-efd698ab491e",
   "metadata": {},
   "source": [
    "**2) DATASET STATISTICS AND VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2f938c-9533-4cab-89c8-eb5ae67c0aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Indices: {'No_Defect': 0, 'line': 1, 'stain': 2, 'tear': 3}\n",
      "Number of Classes: 4\n",
      "Number of Classes: 4\n",
      "Counter({3: 1065, 0: 1053, 1: 1014, 2: 475})\n",
      "Counter({0: 350, 1: 349, 3: 347, 2: 158})\n",
      "Length of traingen: 91\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "train_set= train_generator\n",
    "test_set= test_generator\n",
    "print(\"Class Indices:\", train_set.class_indices)\n",
    "print(\"Number of Classes:\", train_set.num_classes)\n",
    "print(\"Number of Classes:\", test_set.num_classes)\n",
    "from collections import Counter\n",
    "print(Counter(train_set.classes))\n",
    "print(Counter(test_set.classes))\n",
    "print(\"Length of traingen:\",len(train_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab2cc7-93c0-4535-a5cb-6974801cb9ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Map index to class name\n",
    "from collections import defaultdict\n",
    "index_to_class = {v: k for k, v in train_generator.class_indices.items()}\n",
    "\n",
    "# Create a new dictionary to store paths by actual class folder\n",
    "class_to_images = defaultdict(list)\n",
    "\n",
    "# Collect images directly from class folders to ensure correct visualization\n",
    "for class_name, class_idx in train_generator.class_indices.items():\n",
    "    class_dir = os.path.join(train_dir, class_name)\n",
    "    image_paths = glob(os.path.join(class_dir, \"*\"))[:10]  # Get first 10 images\n",
    "    \n",
    "    print(f\"Class: {class_name} ({len(image_paths)} images sampled)\")\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for i, path in enumerate(image_paths):\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, train_generator.img_size)\n",
    "        \n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f\"{class_name}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e00ca-165a-41c4-804b-c0a6be1943f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "# Reverse mapping from index to class name\n",
    "index_to_class = {v: k for k, v in train_generator.class_indices.items()}\n",
    "\n",
    "# Store image paths by class\n",
    "class_to_images = defaultdict(list)\n",
    "\n",
    "for class_name, class_idx in train_generator.class_indices.items():\n",
    "    class_dir = os.path.join(train_dir, class_name)\n",
    "    all_images = glob(os.path.join(class_dir, \"*\"))\n",
    "    \n",
    "    # Randomly sample 100 images\n",
    "    image_paths = random.sample(all_images, min(100, len(all_images)))\n",
    "    class_to_images[class_name] = image_paths\n",
    "    \n",
    "    print(f\"Class: {class_name} ({len(image_paths)} images sampled)\")\n",
    "\n",
    "    # Show all 100 images in a 10x10 grid\n",
    "    plt.figure(figsize=(20, 20))  # Adjust size for clarity\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, train_generator.img_size)\n",
    "        \n",
    "        plt.subplot(10, 10, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f\"{class_name}\", fontsize=6)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027ab96-3201-4504-ba90-748856331dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Map index to class name\n",
    "index_to_class = {v: k for k, v in train_generator.class_indices.items()}\n",
    "\n",
    "# Function to apply the same preprocessing as in your generator\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, train_generator.img_size)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Sobel edge detection\n",
    "    sobelx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    \n",
    "    # Normalize edges to 0-1\n",
    "    sobelx = cv2.normalize(sobelx, None, 0, 1, cv2.NORM_MINMAX)\n",
    "    sobely = cv2.normalize(sobely, None, 0, 1, cv2.NORM_MINMAX)\n",
    "    \n",
    "    return img, sobelx, sobely\n",
    "\n",
    "# Number of images to display per class\n",
    "num_images = 4\n",
    "\n",
    "# Loop through each class\n",
    "for class_name, class_idx in train_generator.class_indices.items():\n",
    "    class_dir = os.path.join(train_dir, class_name)\n",
    "    image_paths = glob(os.path.join(class_dir, \"*\"))[:num_images]  # Get first 4 images\n",
    "    \n",
    "    print(f\"Class: {class_name} ({len(image_paths)} images)\")\n",
    "    \n",
    "    # Create a figure with 3 rows (original, sobelx, sobely) and num_images columns\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    for i, path in enumerate(image_paths):\n",
    "        img, sobelx, sobely = preprocess_image(path)\n",
    "        \n",
    "        # Plot original grayscale image\n",
    "        plt.subplot(3, num_images, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        if i == 0:\n",
    "            plt.ylabel('Original', fontsize=14)\n",
    "        plt.title(f\"{class_name} - {i+1}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Plot Sobel X\n",
    "        \n",
    "        plt.subplot(3, num_images, i + 1 + num_images)\n",
    "        plt.imshow(sobelx, cmap='gray')\n",
    "        if i == 0:\n",
    "            plt.ylabel('Sobel X', fontsize=14)\n",
    "        plt.title(f\"Sobel X-{class_name}-{i+1}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Plot Sobel Y\n",
    "        \n",
    "        plt.subplot(3, num_images, i + 1 + 2*num_images)\n",
    "        plt.imshow(sobely, cmap='gray')\n",
    "        if i == 0:\n",
    "            plt.ylabel('Sobel Y', fontsize=14)\n",
    "        plt.title(f\"Sobel Y-{class_name}-{i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae91f23-e45f-4434-953d-4f20b36b62c1",
   "metadata": {},
   "source": [
    "***MODEL CREATION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "829135d0-f092-47ec-b652-b7a1418f1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "INPUT_SHAPE = (256, 256, 1)  # Input size of images\n",
    "NUM_CLASSES =  train_generator.num_classes             # many class\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 100                 # Increased epochs\n",
    "BATCH_SIZE = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4712afe2-9f9e-414d-bb35-5dcb1d240143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Step 2: Define CNN Model\n",
    "\n",
    "def create_cnn_model(input_shape=(128, 128, 1), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Creates the FN-Net model as described in the paper.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Tuple of (height, width, channels). Default is (128, 128, 1) for grayscale\n",
    "        num_classes: Number of output classes. Default is 2 for binary classification\n",
    "        \n",
    "    Returns:\n",
    "        Keras Sequential model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First Convolutional Block\n",
    "        Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(3, 3), strides=3),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(3, 3), strides=3),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "        \n",
    "        # Fourth Convolutional Block\n",
    "        Conv2D(96, (3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "        \n",
    "        # Flatten layer\n",
    "        Flatten(),\n",
    "        \n",
    "        # First Dense Layer with Dropout\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Second Dense Layer\n",
    "        Dense(256, activation='relu'),\n",
    "        \n",
    "        # Output Layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb7309-caa1-4653-95ab-8ecb02d936c0",
   "metadata": {},
   "source": [
    "**QUANTIZATION AWARE TRAINING (QAT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbcc43-2364-4166-ac4d-abc1f004dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define quantization functions\n",
    "def fake_quantize(x, bits=8, min_value=None, max_value=None):\n",
    "    # Your existing implementation...\n",
    "    # (No changes needed here)\n",
    "    if min_value is None:\n",
    "        min_value = tf.reduce_min(x)\n",
    "    if max_value is None:\n",
    "        max_value = tf.reduce_max(x)\n",
    "    \n",
    "    # Ensure min doesn't equal max to prevent division by zero\n",
    "    max_value = tf.maximum(max_value, min_value + 1e-6)\n",
    "    \n",
    "    # Calculate the step size (the value of 1 bit)\n",
    "    step = (max_value - min_value) / (2**bits - 1)\n",
    "    \n",
    "    # Quantize the values\n",
    "    x_int = tf.round((x - min_value) / step)\n",
    "    \n",
    "    # Clip values to the quantization range\n",
    "    x_int = tf.clip_by_value(x_int, 0, 2**bits - 1)\n",
    "    \n",
    "    # Convert back to original range\n",
    "    x_q = x_int * step + min_value\n",
    "    \n",
    "    # During training, pass through the gradients using STE (Straight Through Estimator)\n",
    "    return x + tf.stop_gradient(x_q - x)\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class QuantizedConv2D(tf.keras.layers.Layer):\n",
    "    # Your existing implementation...\n",
    "    def __init__(self, filters, kernel_size, padding='same', strides=1, activation=None, weight_bits=8, activation_bits=8, **kwargs):\n",
    "        super(QuantizedConv2D, self).__init__(**kwargs)\n",
    "        # self.filters = filters\n",
    "        # self.kernel_size = tuple(kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size))\n",
    "        # self.padding = padding\n",
    "        # self.strides = strides if isinstance(strides, tuple) else (strides, strides)\n",
    "        # self.activation_fn = tf.keras.activations.get(activation)\n",
    "        # self.weight_bits = weight_bits\n",
    "        # self.activation_bits = activation_bits\n",
    "        self.filters = int(filters)\n",
    "        # Ensure kernel_size is stored as a tuple of integers\n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel_size = (int(kernel_size), int(kernel_size))\n",
    "        else:\n",
    "            self.kernel_size = (int(kernel_size[0]), int(kernel_size[1]))\n",
    "        self.padding = padding\n",
    "        # Ensure strides is stored as a tuple of integers\n",
    "        if isinstance(strides, int):\n",
    "            self.strides = (int(strides), int(strides))\n",
    "        else:\n",
    "            self.strides = (int(strides[0]), int(strides[1]))\n",
    "        self.activation_fn = tf.keras.activations.get(activation)\n",
    "        self.weight_bits = int(weight_bits)\n",
    "        self.activation_bits = int(activation_bits)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Your existing build method...\n",
    "        #input_channels = input_shape[-1]\n",
    "        input_channels = int(input_shape[-1])\n",
    "        \n",
    "        #kernel_shape = self.kernel_size + (input_channels, self.filters)\n",
    "        kernel_size = tuple(self.kernel_size)\n",
    "        kernel_shape = kernel_shape = (\n",
    "            int(self.kernel_size[0]),\n",
    "            int(self.kernel_size[1]),\n",
    "            int(input_channels),\n",
    "            int(self.filters)\n",
    "            )\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=kernel_shape,\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.bias = self.add_weight(\n",
    "            name='bias',\n",
    "            shape=(self.filters,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        # Track min and max values for weights (needed for quantization)\n",
    "        self.w_min = self.add_weight(\n",
    "            name='w_min',\n",
    "            shape=(1,),\n",
    "            initializer=tf.constant_initializer(-1.0),\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        self.w_max = self.add_weight(\n",
    "            name='w_max',\n",
    "            shape=(1,),\n",
    "            initializer=tf.constant_initializer(1.0),\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        # Track min and max values for activations\n",
    "        self.a_min = self.add_weight(\n",
    "            name='a_min',\n",
    "            shape=(1,),\n",
    "            initializer=tf.constant_initializer(0.0),\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        self.a_max = self.add_weight(\n",
    "            name='a_max',\n",
    "            shape=(1,),\n",
    "            initializer=tf.constant_initializer(6.0),  # ReLU typically has max around 6\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Your existing call method...\n",
    "        # Update min/max tracking during training\n",
    "        if training:\n",
    "            curr_w_min = tf.reduce_min(self.kernel)\n",
    "            curr_w_max = tf.reduce_max(self.kernel)\n",
    "            \n",
    "            # Use EMA (Exponential Moving Average) to update min/max values\n",
    "            momentum = 0.9\n",
    "            self.w_min.assign(momentum * self.w_min + (1 - momentum) * curr_w_min)\n",
    "            self.w_max.assign(momentum * self.w_max + (1 - momentum) * curr_w_max)\n",
    "        \n",
    "        # Quantize weights\n",
    "        quantized_kernel = fake_quantize(\n",
    "            self.kernel, \n",
    "            bits=self.weight_bits,\n",
    "            min_value=self.w_min,\n",
    "            max_value=self.w_max\n",
    "        )\n",
    "        \n",
    "        # Standard convolution with quantized weights\n",
    "        outputs = tf.nn.conv2d(\n",
    "            inputs,\n",
    "            quantized_kernel,\n",
    "            strides=[1, self.strides[0], self.strides[1], 1],\n",
    "            padding=self.padding.upper()\n",
    "        )\n",
    "        \n",
    "        outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "        \n",
    "        # Apply activation if specified\n",
    "        if self.activation_fn is not None:\n",
    "            outputs = self.activation_fn(outputs)\n",
    "            \n",
    "            # Update activation min/max during training\n",
    "            if training:\n",
    "                curr_a_min = tf.reduce_min(outputs)\n",
    "                curr_a_max = tf.reduce_max(outputs)\n",
    "                \n",
    "                # Use EMA to update min/max values\n",
    "                momentum = 0.9\n",
    "                self.a_min.assign(momentum * self.a_min + (1 - momentum) * curr_a_min)\n",
    "                self.a_max.assign(momentum * self.a_max + (1 - momentum) * curr_a_max)\n",
    "            \n",
    "            # Quantize activations\n",
    "            outputs = fake_quantize(\n",
    "                outputs,\n",
    "                bits=self.activation_bits,\n",
    "                min_value=self.a_min,\n",
    "                max_value=self.a_max\n",
    "            )\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    # def get_config(self):\n",
    "    #     config = super(QuantizedConv2D, self).get_config()\n",
    "    #     config.update({\n",
    "    #         'filters': self.filters,\n",
    "    #         'kernel_size': self.kernel_size,\n",
    "    #         'padding': self.padding,\n",
    "    #         'strides': self.strides,\n",
    "    #         'activation': tf.keras.activations.serialize(self.activation_fn),\n",
    "    #         'weight_bits': self.weight_bits,\n",
    "    #         'activation_bits': self.activation_bits\n",
    "    #     })\n",
    "    #     return config\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(QuantizedConv2D, self).get_config()\n",
    "        config.update({\n",
    "            'filters': int(self.filters),  # Ensure filters is serialized as int\n",
    "            'kernel_size': (int(self.kernel_size[0]), int(self.kernel_size[1])),  # Ensure tuple of ints\n",
    "            'padding': self.padding,\n",
    "            'strides': (int(self.strides[0]), int(self.strides[1])),  # Ensure tuple of ints\n",
    "            'activation': tf.keras.activations.serialize(self.activation_fn),\n",
    "            'weight_bits': int(self.weight_bits),\n",
    "            'activation_bits': int(self.activation_bits)\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class QuantizedDense(tf.keras.layers.Layer):\n",
    "    # Your existing implementation...\n",
    "    def __init__(self, units, activation=None, weight_bits=8, activation_bits=8, **kwargs):\n",
    "        super(QuantizedDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation_fn = tf.keras.activations.get(activation)\n",
    "        self.weight_bits = weight_bits\n",
    "        self.activation_bits = activation_bits\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        \n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=(input_dim, self.units),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.bias = self.add_weight(\n",
    "            name='bias',\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        # Track min and max values for weights\n",
    "        self.w_min = self.add_weight(\n",
    "            name='w_min',\n",
    "            shape=(1,),\n",
    "            initializer=tf.constant_initializer(-1.0),\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        self.w_max = self.add_weight(\n",
    "            name='w_max',\n",
    "            shape=(1,),\n",
    "            initializer=tf.constant_initializer(1.0),\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        # Track min and max values for activations\n",
    "        self.a_min = self.add_weight(\n",
    "            name='a_min',\n",
    "            shape=(1,),\n",
    "            initializer=tf.constant_initializer(0.0),\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        self.a_max = self.add_weight(\n",
    "            name='a_max',\n",
    "            shape=(1,),\n",
    "            initializer=tf.constant_initializer(6.0),\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Your existing call method...\n",
    "        # Update min/max tracking during training\n",
    "        if training:\n",
    "            curr_w_min = tf.reduce_min(self.kernel)\n",
    "            curr_w_max = tf.reduce_max(self.kernel)\n",
    "            \n",
    "            # Use EMA to update min/max values\n",
    "            momentum = 0.9\n",
    "            self.w_min.assign(momentum * self.w_min + (1 - momentum) * curr_w_min)\n",
    "            self.w_max.assign(momentum * self.w_max + (1 - momentum) * curr_w_max)\n",
    "        \n",
    "        # Quantize weights\n",
    "        quantized_kernel = fake_quantize(\n",
    "            self.kernel,\n",
    "            bits=self.weight_bits,\n",
    "            min_value=self.w_min,\n",
    "            max_value=self.w_max\n",
    "        )\n",
    "        \n",
    "        # Standard dense operation with quantized weights\n",
    "        outputs = tf.matmul(inputs, quantized_kernel)\n",
    "        outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "        \n",
    "        # Apply activation if specified\n",
    "        if self.activation_fn is not None:\n",
    "            outputs = self.activation_fn(outputs)\n",
    "            \n",
    "            # Update activation min/max during training\n",
    "            if training:\n",
    "                curr_a_min = tf.reduce_min(outputs)\n",
    "                curr_a_max = tf.reduce_max(outputs)\n",
    "                \n",
    "                # Use EMA to update min/max values\n",
    "                momentum = 0.9\n",
    "                self.a_min.assign(momentum * self.a_min + (1 - momentum) * curr_a_min)\n",
    "                self.a_max.assign(momentum * self.a_max + (1 - momentum) * curr_a_max)\n",
    "            \n",
    "            # Quantize activations\n",
    "            outputs = fake_quantize(\n",
    "                outputs,\n",
    "                bits=self.activation_bits,\n",
    "                min_value=self.a_min,\n",
    "                max_value=self.a_max\n",
    "            )\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(QuantizedDense, self).get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "            'activation': tf.keras.activations.serialize(self.activation_fn),\n",
    "            'weight_bits': self.weight_bits,\n",
    "            'activation_bits': self.activation_bits\n",
    "        })\n",
    "        return config\n",
    "# Now redefine your model creation function using quantized layers\n",
    "def create_quantized_cnn_model(input_shape=(128, 128, 1), num_classes=10, weight_bits=8, activation_bits=8):\n",
    "    \"\"\"\n",
    "    Creates a quantization-aware version of the model\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    x = QuantizedConv2D(16, (3, 3), padding='same', activation='relu', \n",
    "                        weight_bits=weight_bits, activation_bits=activation_bits)(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(6, 6), strides=6)(x)\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    x = QuantizedConv2D(32, (3, 3), padding='same', activation='relu',\n",
    "                        weight_bits=weight_bits, activation_bits=activation_bits)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=3)(x)\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    x = QuantizedConv2D(64, (3, 3), padding='same', activation='relu',\n",
    "                        weight_bits=weight_bits, activation_bits=activation_bits)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "    \n",
    "    # Fourth Convolutional Block\n",
    "    x = QuantizedConv2D(96, (3, 3), padding='same', activation='relu',\n",
    "                        weight_bits=weight_bits, activation_bits=activation_bits)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "    \n",
    "    # Flatten layer\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # First Dense Layer with Dropout\n",
    "    x = QuantizedDense(512, activation='relu',\n",
    "                      weight_bits=weight_bits, activation_bits=activation_bits)(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Second Dense Layer\n",
    "    x = QuantizedDense(256, activation='relu',\n",
    "                      weight_bits=weight_bits, activation_bits=activation_bits)(x)\n",
    "    \n",
    "    # Output Layer - we typically don't quantize the final layer\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Then load the model with custom_objects\n",
    "custom_objects = {\n",
    "    'QuantizedConv2D': QuantizedConv2D,\n",
    "    'QuantizedDense': QuantizedDense\n",
    "}\n",
    "\n",
    "# Load the model\n",
    "#cnn = tf.keras.models.load_model('GreyColor3_Quant.keras', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224deeb-b1bd-43cb-8a9c-6c2ba26b836a",
   "metadata": {},
   "source": [
    "**OPTIMIZERS AND CALLBACK DEFINITIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81557a48-4611-4112-a3a0-8db571379bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_conv2d_20             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">164</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QuantizedConv2D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_conv2d_21             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,644</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QuantizedConv2D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_conv2d_22             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,500</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QuantizedConv2D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_conv2d_23             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">55,396</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QuantizedConv2D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_dense_10              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,836</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QuantizedDense</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_dense_11              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,796</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QuantizedDense</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,028</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_conv2d_20             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m164\u001b[0m │\n",
       "│ (\u001b[38;5;33mQuantizedConv2D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_20 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_conv2d_21             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,644\u001b[0m │\n",
       "│ (\u001b[38;5;33mQuantizedConv2D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_21 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_conv2d_22             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,500\u001b[0m │\n",
       "│ (\u001b[38;5;33mQuantizedConv2D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_22 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_conv2d_23             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │        \u001b[38;5;34m55,396\u001b[0m │\n",
       "│ (\u001b[38;5;33mQuantizedConv2D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_23 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_dense_10              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m24,836\u001b[0m │\n",
       "│ (\u001b[38;5;33mQuantizedDense\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ quantized_dense_11              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,796\u001b[0m │\n",
       "│ (\u001b[38;5;33mQuantizedDense\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │         \u001b[38;5;34m1,028\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">170,364</span> (665.48 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m170,364\u001b[0m (665.48 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">170,340</span> (665.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m170,340\u001b[0m (665.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> (96.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m24\u001b[0m (96.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the model\n",
    "model = create_quantized_cnn_model(INPUT_SHAPE, NUM_CLASSES, weight_bits=8, activation_bits=8)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',  # Multi-class classification\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c9d1a03-e14d-4e62-a729-3f0c6573a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "class StopAtAccuracy(Callback):\n",
    "    def __init__(self, target_acc=0.85):\n",
    "        super(StopAtAccuracy, self).__init__()\n",
    "        self.target_acc = target_acc\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            return\n",
    "        if logs.get(\"val_accuracy\") >= self.target_acc:  # Stop when val_accuracy reaches target\n",
    "            print(f\"\\nStopping training: Reached {self.target_acc * 100:.1f}% validation accuracy\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0a1cd07-154e-4738-be0b-898e1375ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Callbacks\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=9, restore_best_weights=True, verbose=1\n",
    ")\n",
    "stop_at_100 = StopAtAccuracy(target_acc=1)\n",
    "# Learning rate scheduler to reduce LR when validation loss plateaus\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "# Model checkpoint to save the best model\n",
    "#checkpoint = ModelCheckpoint('/mnt/c/modelFiles/GreyColor3_Quant_kfold_old_learn.keras', monitor='val_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf1abd-d020-43f5-abfb-1c61ed4de275",
   "metadata": {},
   "source": [
    "***MODEL TRAINING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cc505cc-a8e7-443e-8cc3-696ce408ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srihari2895/MiniProj/mpenv/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 18:04:01.709666: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2011', 232 bytes spill stores, 232 bytes spill loads\n",
      "\n",
      "2025-05-04 18:04:01.786937: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2011', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2025-05-04 18:04:01.954629: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2011', 248 bytes spill stores, 248 bytes spill loads\n",
      "\n",
      "2025-05-04 18:04:01.974367: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2011', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 75/109\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 927ms/step - accuracy: 0.3737 - loss: 1.2387"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 18:05:14.686183: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2009', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-04 18:05:14.830271: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2009', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931ms/step - accuracy: 0.3992 - loss: 1.1891\n",
      "Epoch 1: val_loss improved from inf to 0.97245, saving model to /mnt/c/modelFiles/10splits/GreyColor3_Quant_kfold_old_learn_0.keras\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.3999 - loss: 1.1878 - val_accuracy: 0.5457 - val_loss: 0.9725 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5762 - loss: 0.8701\n",
      "Epoch 2: val_loss improved from 0.97245 to 0.69531, saving model to /mnt/c/modelFiles/10splits/GreyColor3_Quant_kfold_old_learn_0.keras\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - accuracy: 0.5764 - loss: 0.8697 - val_accuracy: 0.6611 - val_loss: 0.6953 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m 92/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m12s\u001b[0m 728ms/step - accuracy: 0.6459 - loss: 0.7245"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/c/modelFiles/10splits/GreyColor3_Quant_kfold_old_learn_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_at_100\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/c/modelFiles/10splits/greyColor_quantized_kfold_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/MiniProj/mpenv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "history_list = []\n",
    "i = 0\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(all_filepaths, all_labels)):\n",
    "    print(f\"\\n[INFO] Fold {fold + 1}\")\n",
    "\n",
    "    X_train, y_train = all_filepaths[train_idx], all_labels[train_idx]\n",
    "    X_val, y_val = all_filepaths[val_idx], all_labels[val_idx]\n",
    "\n",
    "    # Create data generators for this fold\n",
    "    train_generator = CustomDataGenerator(X_train, y_train, BATCH_SIZE, IMG_SIZE, NUM_CLASSES, shuffle=True, class_indices=class_indices)\n",
    "    val_generator = CustomDataGenerator(X_val, y_val, BATCH_SIZE, IMG_SIZE, NUM_CLASSES, shuffle=False, class_indices=class_indices)\n",
    "\n",
    "    # Initialize model (make sure create_model is defined)\n",
    "    model = create_quantized_cnn_model(INPUT_SHAPE, NUM_CLASSES, weight_bits=8, activation_bits=8)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    with tf.device('/GPU:0'):\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=test_generator,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[early_stopping, lr_scheduler, ModelCheckpoint(f'/mnt/c/modelFiles/10splits/GreyColor3_Quant_kfold_old_learn_{fold}.keras', monitor='val_loss', save_best_only=True, verbose=1), stop_at_100],\n",
    "            verbose=1\n",
    "        )\n",
    "    model.save(f\"/mnt/c/modelFiles/10splits/greyColor_quantized_kfold_model_{fold}.keras\")\n",
    "    if i == 0:\n",
    "        model.summary()\n",
    "        i += 1\n",
    "    history_list.append(history)\n",
    "    model.save(f\"/mnt/c/modelFiles/10splits/qat_kfold_model_{fold}.h5\")  # Save in HDF5 format\n",
    "    print(\"model saved\")\n",
    "    # Reload it (optional, just to make sure conversion works from disk)\n",
    "    modelh5 = tf.keras.models.load_model(f\"/mnt/c/modelFiles/10splits/qat_kfold_model_{fold}.h5\", custom_objects=custom_objects)\n",
    "    print(\"model loaded\")\n",
    "    # Convert to TensorFlow Lite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(modelh5)\n",
    "    print(\"converter set\")\n",
    "    # Enable full integer quantization\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    print(\"optimizations initialized\")\n",
    "    # Define a representative dataset function for calibration\n",
    "    def representative_dataset_gen():\n",
    "        for i in range(100):  # Just use first 100 images for calibration\n",
    "            img, _ = train_generator[i]\n",
    "            for x in img:\n",
    "                yield [np.expand_dims(x, axis=0).astype(np.float32)]\n",
    "\n",
    "\n",
    "    converter.representative_dataset = representative_dataset_gen\n",
    "    print(\"representative dataset initialized\")\n",
    "    # Ensure all tensors are int8\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    print(\"type bit converstions done\")\n",
    "    # Perform conversion\n",
    "    quantized_tflite_model = converter.convert()\n",
    "    print(\"quantized model obtained\")\n",
    "    # Save the quantized model to disk\n",
    "    with open(f\"/mnt/c/modelFiles/10splits/quantized_kfold_model_{fold}.tflite\", \"wb\") as f:\n",
    "        f.write(quantized_tflite_model)\n",
    "    print(\"tfmodel saved\") #tflite *\n",
    "\n",
    "    # Optionally save the model\n",
    "    # model.save(f'model_fold_{fold + 1}.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429617ca-96cf-43ba-94e0-9c899ad64a82",
   "metadata": {},
   "source": [
    "**MODEL PERFORMANCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f737da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a list of history objects called history_list\n",
    "# Each history object contains the training history from one run\n",
    "\n",
    "def plot_twenty_individual_graphs(history_list):\n",
    "    \"\"\"\n",
    "    Creates 10 separate plots:\n",
    "    - 5 plots for accuracy vs validation accuracy (one for each run)\n",
    "    - 5 plots for loss vs validation loss (one for each run)\n",
    "    \"\"\"\n",
    "    # Verify we have 5 history objects\n",
    "    if len(history_list) != 10:\n",
    "        print(f\"Warning: Expected 10 history objects, but found {len(history_list)}\")\n",
    "    \n",
    "    # Create figure for accuracy plots (top row)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Plot accuracy vs val_accuracy for each run\n",
    "    for i, history in enumerate(history_list):\n",
    "        # Get the history dictionary\n",
    "        history_dict = history.history\n",
    "        \n",
    "        # Check for accuracy metric name (could be 'acc' or 'accuracy')\n",
    "        acc_key = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_key = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Get the number of epochs\n",
    "        epochs = range(1, len(history_dict[acc_key]) + 1)\n",
    "        \n",
    "        # Create subplot for this run's accuracy\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.plot(epochs, history_dict[acc_key], 'b-', label='Training Accuracy')\n",
    "        plt.plot(epochs, history_dict[val_acc_key], 'r-', label='Validation Accuracy')\n",
    "        plt.title(f'Run {i+1} Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"/mnt/c/modelFiles/10splits/accuracy_plots.png\")\n",
    "    plt.show()\n",
    "    print(\"Accuracy plots saved as 'accuracy_plots.png'\")\n",
    "    \n",
    "    # Create figure for loss plots (bottom row)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Plot loss vs val_loss for each run\n",
    "    for i, history in enumerate(history_list):\n",
    "        # Get the history dictionary\n",
    "        history_dict = history.history\n",
    "        \n",
    "        # Get the number of epochs\n",
    "        epochs = range(1, len(history_dict['loss']) + 1)\n",
    "        \n",
    "        # Create subplot for this run's loss\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.plot(epochs, history_dict['loss'], 'b-', label='Training Loss')\n",
    "        plt.plot(epochs, history_dict['val_loss'], 'r-', label='Validation Loss')\n",
    "        plt.title(f'Run {i+1} Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"/mnt/c/modelFiles/10splits/loss_plots.png\")\n",
    "    plt.show()\n",
    "    print(\"Loss plots saved as 'loss_plots.png'\")\n",
    "\n",
    "    # Optional: Print summary statistics for each run\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    for i, history in enumerate(history_list):\n",
    "        history_dict = history.history\n",
    "        acc_key = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_key = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        final_train_acc = history_dict[acc_key][-1]\n",
    "        final_val_acc = history_dict[val_acc_key][-1]\n",
    "        final_train_loss = history_dict['loss'][-1]\n",
    "        final_val_loss = history_dict['val_loss'][-1]\n",
    "        \n",
    "        print(f\"Run {i+1}:\")\n",
    "        print(f\"  Final training accuracy: {final_train_acc:.4f}\")\n",
    "        print(f\"  Final validation accuracy: {final_val_acc:.4f}\")\n",
    "        print(f\"  Final training loss: {final_train_loss:.4f}\")\n",
    "        print(f\"  Final validation loss: {final_val_loss:.4f}\")\n",
    "\n",
    "# Call the function with your list of history objects\n",
    "plot_twenty_individual_graphs(history_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf90d7-8aaf-429f-a6b8-8a7871b5b80f",
   "metadata": {},
   "source": [
    "**QUANTIZED MODEL EVALUATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c571a054",
   "metadata": {},
   "source": [
    "Soft Voting Ensemble of 5 .tflite Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Paths to the 5 TFLite models\n",
    "tflite_model_paths = [f\"/mnt/c/modelFiles/try2_quantized_kfold_model_{i}.tflite\" for i in range(5)]\n",
    "\n",
    "# Load all interpreters\n",
    "interpreters = []\n",
    "input_details_list = []\n",
    "output_details_list = []\n",
    "quant_params_list = []\n",
    "\n",
    "for path in tflite_model_paths:\n",
    "    interpreter = tf.lite.Interpreter(model_path=path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    scale, zero_point = input_details[0]['quantization']\n",
    "\n",
    "    interpreters.append(interpreter)\n",
    "    input_details_list.append(input_details)\n",
    "    output_details_list.append(output_details)\n",
    "    quant_params_list.append((scale, zero_point))\n",
    "\n",
    "# Ensemble prediction\n",
    "y_true = []\n",
    "y_pred_ensemble = []\n",
    "\n",
    "print(f\"\\nRunning ensemble prediction using {len(interpreters)} TFLite models...\\n\")\n",
    "\n",
    "for batch_images, batch_labels in test_generator:\n",
    "    for img, label in zip(batch_images, batch_labels):\n",
    "        img = np.expand_dims(img, axis=0).astype(np.float32)  # [1, H, W, C]\n",
    "        \n",
    "        # Collect predictions from all models\n",
    "        preds = []\n",
    "        for i in range(len(interpreters)):\n",
    "            scale, zero_point = quant_params_list[i]\n",
    "            img_int8 = (img / scale + zero_point).astype(np.int8)\n",
    "\n",
    "            interpreters[i].set_tensor(input_details_list[i][0]['index'], img_int8)\n",
    "            interpreters[i].invoke()\n",
    "            output_data = interpreters[i].get_tensor(output_details_list[i][0]['index'])\n",
    "\n",
    "            preds.append(output_data[0])  # shape: (NUM_CLASSES,)\n",
    "\n",
    "        # Average predictions\n",
    "        avg_pred = np.mean(preds, axis=0)\n",
    "        pred_class = np.argmax(avg_pred)\n",
    "        true_class = np.argmax(label)\n",
    "\n",
    "        y_pred_ensemble.append(pred_class)\n",
    "        y_true.append(true_class)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"TFLite Ensemble Model Accuracy:\", np.mean(np.array(y_true) == np.array(y_pred_ensemble)))\n",
    "print(\"\\nClassification Report (Soft-Voted Ensemble):\")\n",
    "target_names = list(label_encoder.classes_)  # assumes you have a fitted label_encoder\n",
    "print(classification_report(y_true, y_pred_ensemble, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b97049-52ff-4346-be2f-2283056fcb56",
   "metadata": {},
   "source": [
    "***KERAS MODEL EVALUATION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd21f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load all 5 models\n",
    "model_paths = [f\"/mnt/c/modelFiles/greyColor_quantized_kfold_model_{fold}.keras\" for i in range(5)]\n",
    "models = [load_model(path) for path in model_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dafb6c-96c8-4f2e-9320-976c26cc9ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_soft_voting(models, test_generator):\n",
    "    num_samples = len(test_generator.image_paths)\n",
    "    steps = int(np.ceil(num_samples / test_generator.batch_size))\n",
    "\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x_batch, y_batch = next(test_generator)\n",
    "        pred_probs = np.zeros((len(x_batch), len(test_generator.class_indices)))\n",
    "\n",
    "        for model in models:\n",
    "            pred_probs += model.predict(x_batch, verbose=0)\n",
    "\n",
    "        avg_probs = pred_probs / len(models)\n",
    "        y_pred_all.extend(np.argmax(avg_probs, axis=1))\n",
    "        y_true_all.extend(np.argmax(y_batch, axis=1))\n",
    "\n",
    "    y_true_all = np.array(y_true_all[:num_samples])\n",
    "    y_pred_all = np.array(y_pred_all[:num_samples])\n",
    "\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix - Soft Voting Ensemble')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Metrics\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true_all, y_pred_all, target_names=class_names))\n",
    "\n",
    "    accuracy = np.sum(y_true_all == y_pred_all) / len(y_true_all)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-class Accuracy:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask = y_true_all == i\n",
    "        correct = np.sum((y_true_all == i) & (y_pred_all == i))\n",
    "        print(f\"{class_name}: {correct / np.sum(mask):.4f}\")\n",
    "\n",
    "# Usage\n",
    "evaluate_soft_voting(models, test_generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33922a33-7bb6-4bbc-ae81-daa0e0ff7614",
   "metadata": {},
   "source": [
    "(SIMULATION) \n",
    "**COMPARISON BETWEEN A GENERIC MODEL AND QUANTIZATION AWARE TRAINING MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5582369-db62-410c-ab58-3b912fb33fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_quantization_effects(original_model_path, quantized_model_path, test_generator):\n",
    "    \"\"\"\n",
    "    Visualize the effects of quantization on model weights and activations.\n",
    "    \n",
    "    Args:\n",
    "        original_model_path: Path to the original (non-quantized) model\n",
    "        quantized_model_path: Path to the quantized-aware trained model\n",
    "        test_generator: A data generator providing test data\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Define custom objects for loading the quantized model\n",
    "    custom_objects = {'QuantizedConv2D': QuantizedConv2D, 'QuantizedDense': QuantizedDense}\n",
    "    \n",
    "    # Load both models\n",
    "    original_model = tf.keras.models.load_model(original_model_path)\n",
    "    quantized_model = tf.keras.models.load_model(quantized_model_path, custom_objects=custom_objects)\n",
    "    # Build models if they aren't already\n",
    "    sample_input_shape = (None, 256, 256, 1)\n",
    "    original_model.build(input_shape=sample_input_shape)\n",
    "    quantized_model.build(input_shape=sample_input_shape)\n",
    "    # Get a batch of test data\n",
    "    x_batch, _ = next(iter(test_generator))\n",
    "    \n",
    "    # Extract weights for comparison\n",
    "    def get_conv_weights(model):\n",
    "        # Get weights from the first conv layer\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, QuantizedConv2D):\n",
    "                return layer.get_weights()[0]  # Return kernel weights\n",
    "        return None\n",
    "    \n",
    "    orig_weights = get_conv_weights(original_model)\n",
    "    quant_weights = get_conv_weights(quantized_model)\n",
    "    \n",
    "    # 1. Compare weight distributions\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(orig_weights.flatten(), bins=50, alpha=0.5, label='Original')\n",
    "    plt.title('Original Weight Distribution')\n",
    "    plt.xlabel('Weight Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(quant_weights.flatten(), bins=50, alpha=0.5, label='Quantized')\n",
    "    plt.title('Quantized Weight Distribution')\n",
    "    plt.xlabel('Weight Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(orig_weights.flatten(), bins=50, alpha=0.5, label='Original')\n",
    "    plt.hist(quant_weights.flatten(), bins=50, alpha=0.5, label='Quantized')\n",
    "    plt.title('Overlaid Distributions')\n",
    "    plt.xlabel('Weight Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('weight_distribution_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Visualize quantization steps\n",
    "    # Create a simple model to extract activations\n",
    "    def create_activation_model(model, layer_name):\n",
    "        # Define the known input shape (256, 256, 3)\n",
    "        input_shape = (256,256,1)\n",
    "        \n",
    "        # Create new input layer\n",
    "        input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "        \n",
    "        # Find the target layer\n",
    "        target_layer = None\n",
    "        x = input_layer\n",
    "        \n",
    "        for layer in model.layers:\n",
    "            try:\n",
    "                x = layer(x)\n",
    "                if layer_name.lower() in layer.name.lower():  # Case-insensitive match\n",
    "                    target_layer = x\n",
    "                    break\n",
    "            except:\n",
    "                # If layer fails (like Input layers), skip it\n",
    "                continue\n",
    "        \n",
    "        # If target layer not found, use the output of the middle layer\n",
    "        if target_layer is None:\n",
    "            middle_idx = len(model.layers) // 2\n",
    "            x = input_layer\n",
    "            for i, layer in enumerate(model.layers):\n",
    "                try:\n",
    "                    x = layer(x)\n",
    "                    if i == middle_idx:\n",
    "                        target_layer = x\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return tf.keras.Model(inputs=input_layer, outputs=target_layer)\n",
    "    # Get activations from a middle conv layer\n",
    "    orig_activation_model = create_activation_model(original_model, 'conv2d')\n",
    "    quant_activation_model = create_activation_model(quantized_model, 'quantized_conv2d')\n",
    "    \n",
    "    orig_activations = orig_activation_model.predict(x_batch)\n",
    "    quant_activations = quant_activation_model.predict(x_batch)\n",
    "    \n",
    "    # Plot activation distributions\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(orig_activations.flatten(), bins=50, alpha=0.5)\n",
    "    plt.title('Original Activation Distribution')\n",
    "    plt.xlabel('Activation Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(quant_activations.flatten(), bins=50, alpha=0.5)\n",
    "    plt.title('Quantized Activation Distribution')\n",
    "    plt.xlabel('Activation Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(orig_activations.flatten(), bins=50, alpha=0.5, label='Original')\n",
    "    plt.hist(quant_activations.flatten(), bins=50, alpha=0.5, label='Quantized')\n",
    "    plt.title('Overlaid Activation Distributions')\n",
    "    plt.xlabel('Activation Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('activation_distribution_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Visualize original vs quantized features for a sample image\n",
    "    # Get a single test image\n",
    "    sample_img = x_batch[0:1]\n",
    "    \n",
    "    # Function to get intermediate layer outputs\n",
    "    def get_layer_outputs(model, input_data, num_layers=4):\n",
    "           \n",
    "        model.build(input_shape=(None,256, 256, 1))\n",
    "\n",
    "        \n",
    "        outputs = []\n",
    "        count = 0\n",
    "    \n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, (tf.keras.layers.Conv2D, QuantizedConv2D)):\n",
    "                temp_model = tf.keras.Model(inputs=model.inputs, outputs=layer.output)\n",
    "                out = temp_model.predict(input_data)\n",
    "                outputs.append(out)\n",
    "                count += 1\n",
    "                if count >= num_layers:\n",
    "                    break\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    orig_features = get_layer_outputs(original_model, sample_img)\n",
    "    quant_features = get_layer_outputs(quantized_model, sample_img)\n",
    "    \n",
    "    # Plot feature maps from conv layers\n",
    "    for layer_idx in range(min(len(orig_features), len(quant_features))):\n",
    "        orig_feature = orig_features[layer_idx][0]\n",
    "        quant_feature = quant_features[layer_idx][0]\n",
    "        \n",
    "        # Plot first 8 channels from each feature map\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.suptitle(f'Layer {layer_idx+1} Feature Maps', fontsize=16)\n",
    "        \n",
    "        for i in range(min(8, orig_feature.shape[-1])):\n",
    "            # Original feature\n",
    "            plt.subplot(2, 8, i+1)\n",
    "            plt.imshow(orig_feature[:,:,i], cmap='viridis')\n",
    "            plt.title(f'Original Ch {i+1}')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Quantized feature\n",
    "            plt.subplot(2, 8, i+9)\n",
    "            plt.imshow(quant_feature[:,:,i], cmap='viridis')\n",
    "            plt.title(f'Quantized Ch {i+1}')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(f'feature_map_comparison_layer_{layer_idx+1}.png')\n",
    "        plt.show()\n",
    "    \n",
    "    # 4. Compare prediction probabilities\n",
    "    orig_preds = original_model.predict(x_batch[:5])\n",
    "    quant_preds = quantized_model.predict(x_batch[:5])\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i in range(min(5, len(orig_preds))):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        \n",
    "        # Get top 3 class indices\n",
    "        top_orig = np.argsort(orig_preds[i])[-3:][::-1]\n",
    "        top_quant = np.argsort(quant_preds[i])[-3:][::-1]\n",
    "        \n",
    "        # Combine unique classes\n",
    "        classes = np.unique(np.concatenate([top_orig, top_quant]))\n",
    "        \n",
    "        # Plot side by side\n",
    "        x = np.arange(len(classes))\n",
    "        width = 0.35\n",
    "        \n",
    "        orig_values = [orig_preds[i][cls] for cls in classes]\n",
    "        quant_values = [quant_preds[i][cls] for cls in classes]\n",
    "        \n",
    "        plt.bar(x - width/2, orig_values, width, label='Original')\n",
    "        plt.bar(x + width/2, quant_values, width, label='Quantized')\n",
    "        \n",
    "        plt.title(f'Sample {i+1}')\n",
    "        plt.xlabel('Class Index')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.xticks(x, classes)\n",
    "        \n",
    "        if i == 0:\n",
    "            plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_probability_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Calculate size reduction\n",
    "    def get_model_size(model):\n",
    "        \"\"\"Get approximate model size in MB\"\"\"\n",
    "        weights = [w.numpy() for w in model.weights]\n",
    "        total_params = sum(w.size for w in weights)\n",
    "        \n",
    "        # Calculate size in MB (32-bit float = 4 bytes)\n",
    "        size_mb = (total_params * 4) / (1024 * 1024)\n",
    "        return size_mb\n",
    "    \n",
    "    orig_size = get_model_size(original_model)\n",
    "    \n",
    "    # Simulate quantized model size (8-bit = 1 byte)\n",
    "    quant_size = get_model_size(quantized_model) / 4  # Approximation: 8-bit is 1/4 of 32-bit\n",
    "    \n",
    "    print(f\"Original model size: {orig_size:.2f} MB\")\n",
    "    print(f\"Estimated quantized model size: {quant_size:.2f} MB\")\n",
    "    print(f\"Size reduction: {(1 - quant_size/orig_size) * 100:.1f}%\")\n",
    "    \n",
    "    # 6. Measure and compare inference speed\n",
    "    import time\n",
    "    \n",
    "    # Warm up\n",
    "    _ = original_model.predict(x_batch[:10])\n",
    "    _ = quantized_model.predict(x_batch[:10])\n",
    "    \n",
    "    # Measure original model speed\n",
    "    start_time = time.time()\n",
    "    _ = original_model.predict(x_batch)\n",
    "    orig_time = time.time() - start_time\n",
    "    \n",
    "    # Measure quantized model speed\n",
    "    start_time = time.time()\n",
    "    _ = quantized_model.predict(x_batch)\n",
    "    quant_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Original model inference time: {orig_time:.4f} seconds\")\n",
    "    print(f\"Quantized model inference time: {quant_time:.4f} seconds\")\n",
    "    print(f\"Speed improvement: {(1 - quant_time/orig_time) * 100:.1f}%\")\n",
    "    \n",
    "    # Return summary as dictionary for further analysis\n",
    "    return {\n",
    "        \"original_size_mb\": orig_size,\n",
    "        \"quantized_size_mb\": quant_size,\n",
    "        \"size_reduction_percent\": (1 - quant_size/orig_size) * 100,\n",
    "        \"original_inference_time\": orig_time,\n",
    "        \"quantized_inference_time\": quant_time,\n",
    "        \"speed_improvement_percent\": (1 - quant_time/orig_time) * 100\n",
    "    }\n",
    "\n",
    "# Usage example:\n",
    "# First train your model with quantization-aware training\n",
    "# Then call this function:\n",
    "visualize_quantization_effects('/mnt/c/modelFiles/GreyColor_NoQuant_NoKFold.keras', \"/mnt/c/modelFiles/greyColor_quantized_kfold_model_0.keras\", test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bd15e-8293-400f-b110-45f6c4666c84",
   "metadata": {},
   "source": [
    "**CLASS BASED ADAPTIVE ACCURACY THRESHOLDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, roc_curve, auc\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_metrics_multiclass(y_true, y_pred_probs, thresholds):\n",
    "    \"\"\"\n",
    "    Apply per-class thresholds and compute evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True class labels\n",
    "        y_pred_probs: Predicted probabilities for each class\n",
    "        thresholds: List of per-class thresholds\n",
    "    \n",
    "    Returns:\n",
    "        y_pred: Final predicted labels based on adaptive thresholds\n",
    "        cm: Confusion matrix\n",
    "        f1_scores: Per-class F1 scores\n",
    "    \"\"\"\n",
    "    num_classes = y_pred_probs.shape[1]\n",
    "    \n",
    "    # Apply adaptive thresholds: assign class only if it exceeds its threshold\n",
    "    y_pred = np.full(y_true.shape, -1)  # Initialize with -1 (uncertain)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        mask = y_pred_probs[:, i] >= thresholds[i]\n",
    "        y_pred[mask] = i  # Assign class i if its probability exceeds threshold\n",
    "    \n",
    "    # Handle uncertain cases: assign most confident prediction if no threshold is met\n",
    "    uncertain_mask = y_pred == -1\n",
    "    y_pred[uncertain_mask] = np.argmax(y_pred_probs[uncertain_mask], axis=1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Compute per-class F1 scores\n",
    "    f1_scores = f1_score(y_true, y_pred, average=None)  # Per-class F1-score\n",
    "    overall_f1 = f1_score(y_true, y_pred, average=\"macro\")  # Macro-average F1-score\n",
    "    \n",
    "    return y_pred, cm, f1_scores, overall_f1\n",
    "\n",
    "def soft_voting_ensemble(models, test_generator):\n",
    "    \"\"\"\n",
    "    Perform soft voting ensemble prediction from multiple models.\n",
    "    \n",
    "    Args:\n",
    "        models: List of trained models\n",
    "        test_generator: Test data generator\n",
    "    \n",
    "    Returns:\n",
    "        y_true: True class labels\n",
    "        y_pred_probs: Averaged predicted probabilities from ensemble\n",
    "    \"\"\"\n",
    "    steps = len(test_generator)\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        x_batch, y_batch = next(test_generator)\n",
    "        y_true.extend(np.argmax(y_batch, axis=1))  # Convert one-hot labels to indices\n",
    "        \n",
    "        # Get predicted probabilities from each model and average them\n",
    "        model_preds = [model.predict(x_batch, verbose=0) for model in models]\n",
    "        avg_preds = np.mean(model_preds, axis=0)  # Average probabilities\n",
    "        y_pred_probs.extend(avg_preds)\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    \n",
    "    return y_true, y_pred_probs\n",
    "\n",
    "def adaptive_threshold_determination_multiclass_ensemble(models, test_generator, threshold_range=None):\n",
    "    \"\"\"\n",
    "    Determine adaptive per-class thresholds and evaluate the ensemble model.\n",
    "    \n",
    "    Args:\n",
    "        models: List of trained models\n",
    "        test_generator: Test data generator\n",
    "        threshold_range: Range of thresholds to test (default: 0 to 1 in 0.01 steps)\n",
    "    \n",
    "    Returns:\n",
    "        optimal_thresholds: List of per-class optimal thresholds\n",
    "    \"\"\"\n",
    "    # Get ensemble predictions\n",
    "    y_true, y_pred_probs = soft_voting_ensemble(models, test_generator)\n",
    "    \n",
    "    num_classes = y_pred_probs.shape[1]\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "    # Set threshold search range\n",
    "    if threshold_range is None:\n",
    "        threshold_range = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "    # Find best threshold for each class based on F1-score\n",
    "    best_thresholds = []\n",
    "    best_f1_scores = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        best_threshold = 0.5  # Default\n",
    "        best_f1 = 0.0\n",
    "\n",
    "        for threshold in threshold_range:\n",
    "            temp_thresholds = [0.5] * num_classes  # Start with default 0.5 for all\n",
    "            temp_thresholds[i] = threshold  # Vary only the current class\n",
    "            \n",
    "            _, _, f1_scores, _ = calculate_metrics_multiclass(y_true, y_pred_probs, temp_thresholds)\n",
    "            if f1_scores[i] > best_f1:\n",
    "                best_f1 = f1_scores[i]\n",
    "                best_threshold = threshold\n",
    "\n",
    "        best_thresholds.append(best_threshold)\n",
    "        best_f1_scores.append(best_f1)\n",
    "\n",
    "    # Apply optimal thresholds\n",
    "    y_pred, cm, f1_scores, overall_f1 = calculate_metrics_multiclass(y_true, y_pred_probs, best_thresholds)\n",
    "\n",
    "    # Compute ROC Curve & AUC per class\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        y_true_binary = (y_true == i).astype(int)\n",
    "        y_pred_binary = y_pred_probs[:, i]\n",
    "\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_binary, y_pred_binary)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    macro_auc = np.mean(list(roc_auc.values()))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC Curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Multiclass ROC Curve (Macro AUC = {macro_auc:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    # Print optimal thresholds\n",
    "    print(\"\\nOptimal Per-Class Thresholds:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name}: {best_thresholds[i]:.2f} (F1 = {best_f1_scores[i]:.4f})\")\n",
    "\n",
    "    print(f\"\\nOverall Macro F1 Score: {overall_f1:.4f}\")\n",
    "    print(f\"Macro-Averaged ROC AUC: {macro_auc:.4f}\")\n",
    "\n",
    "    return best_thresholds\n",
    "\n",
    "# Usage:\n",
    "def evaluate_multiclass_with_adaptive_threshold_ensemble(models, test_generator):\n",
    "    \"\"\"\n",
    "    Evaluate ensemble model on a multi-class dataset using adaptive thresholding.\n",
    "    \"\"\"\n",
    "    optimal_thresholds = adaptive_threshold_determination_multiclass_ensemble(models, test_generator)\n",
    "    return optimal_thresholds\n",
    "\n",
    "\n",
    "# Call the evaluation function\n",
    "try:\n",
    "    evaluate_multiclass_with_adaptive_threshold_ensemble(models, test_generator)\n",
    "except StopIteration as e:\n",
    "    print(\"Iteration stopped. Evaluating again.\")\n",
    "    evaluate_multiclass_with_adaptive_threshold_ensemble(models, test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26dcff-1569-4c23-9d87-be3d89f2022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# %%\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, threshold):\n",
    "    \"\"\"Calculate TP, TN, FP, FN, FPR, FNR for given threshold\"\"\"\n",
    "    predictions = (y_pred >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, predictions).ravel()   \n",
    "    \n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    f1 = f1_score(y_true, predictions)\n",
    "    \n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn,\n",
    "        'FPR': fpr, 'FNR': fnr, 'F1': f1\n",
    "    }\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9860a7-c2d7-4078-8bfa-200ad1873db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#evaluate multiclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve, auc, classification_report\n",
    "import tensorflow as tf\n",
    "\n",
    "def adaptive_threshold_determination_multiclass1(model, test_generator):\n",
    "    \"\"\"\n",
    "    Evaluate a multi-class classification model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_generator: Test data generator\n",
    "    \"\"\"\n",
    "    # Reset generator and get predictions\n",
    "    test_generator.reset()\n",
    "    steps = len(test_generator)\n",
    "    \n",
    "    # Get all predictions and true labels\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "    \n",
    "    for i in range(steps):\n",
    "        x_batch, y_batch = next(test_generator)\n",
    "        batch_pred = model.predict(x_batch, verbose=0)\n",
    "        y_true.extend(np.argmax(y_batch, axis=1))  # Convert one-hot labels to class indices\n",
    "        y_pred_probs.extend(batch_pred)  # Keep full probability distribution\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    \n",
    "    # Convert predicted probabilities to class labels\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Compute per-class F1 scores\n",
    "    f1_scores = f1_score(y_true, y_pred, average=None)  # Per-class F1-score\n",
    "    overall_f1 = f1_score(y_true, y_pred, average=\"macro\")  # Macro-average F1-score\n",
    "    \n",
    "    # ROC Curve & AUC for multiclass (One-vs-Rest)\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        # Convert labels to binary (One-vs-Rest)\n",
    "        y_true_binary = (y_true == i).astype(int)\n",
    "        y_pred_binary = y_pred_probs[:, i]\n",
    "        \n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_binary, y_pred_binary)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Macro-Averaged AUC\n",
    "    macro_auc = np.mean(list(roc_auc.values()))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC Curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Multiclass ROC Curve (Macro AUC = {macro_auc:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    # Print per-class F1 scores\n",
    "    print(\"\\nPer-Class F1 Scores:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name}: {f1_scores[i]:.4f}\")\n",
    "\n",
    "    print(f\"\\nOverall Macro F1 Score: {overall_f1:.4f}\")\n",
    "    print(f\"Macro-Averaged ROC AUC: {macro_auc:.4f}\")\n",
    "\n",
    "# Usage:\n",
    "def evaluate_multiclass(model, test_generator):\n",
    "    \"\"\"\n",
    "    Evaluate model on a multi-class dataset.\n",
    "    \"\"\"\n",
    "    adaptive_threshold_determination_multiclass1(model, test_generator)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a936ec-6973-4cfb-92bf-ff3449979dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#PER CLASS THRESHOLDING\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_metrics_multiclass(y_true, y_pred_probs, thresholds):\n",
    "    \"\"\"\n",
    "    Apply per-class thresholds and compute evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True class labels\n",
    "        y_pred_probs: Predicted probabilities for each class\n",
    "        thresholds: List of per-class thresholds\n",
    "    \n",
    "    Returns:\n",
    "        y_pred: Final predicted labels based on adaptive thresholds\n",
    "        cm: Confusion matrix\n",
    "        f1_scores: Per-class F1 scores\n",
    "    \"\"\"\n",
    "    num_classes = y_pred_probs.shape[1]\n",
    "    \n",
    "    # Apply adaptive thresholds: assign class only if it exceeds its threshold\n",
    "    y_pred = np.full(y_true.shape, -1)  # Initialize with -1 (uncertain)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        mask = y_pred_probs[:, i] >= thresholds[i]\n",
    "        y_pred[mask] = i  # Assign class i if its probability exceeds threshold\n",
    "    \n",
    "    # Handle uncertain cases: assign most confident prediction if no threshold is met\n",
    "    uncertain_mask = y_pred == -1\n",
    "    y_pred[uncertain_mask] = np.argmax(y_pred_probs[uncertain_mask], axis=1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Compute per-class F1 scores\n",
    "    f1_scores = f1_score(y_true, y_pred, average=None)  # Per-class F1-score\n",
    "    overall_f1 = f1_score(y_true, y_pred, average=\"macro\")  # Macro-average F1-score\n",
    "    \n",
    "    return y_pred, cm, f1_scores, overall_f1\n",
    "\n",
    "\n",
    "def adaptive_threshold_determination_multiclass2(model, test_generator, threshold_range=None):\n",
    "    \"\"\"\n",
    "    Determine adaptive per-class thresholds and evaluate the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_generator: Test data generator\n",
    "        threshold_range: Range of thresholds to test (default: 0 to 1 in 0.01 steps)\n",
    "    \n",
    "    Returns:\n",
    "        optimal_thresholds: List of per-class optimal thresholds\n",
    "    \"\"\"\n",
    "    # Reset generator and get predictions\n",
    "    #test_generator.reset()\n",
    "    steps = len(test_generator)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        x_batch, y_batch = next(test_generator)\n",
    "        batch_pred = model.predict(x_batch, verbose=0)\n",
    "        y_true.extend(np.argmax(y_batch, axis=1))  # Convert one-hot labels to indices\n",
    "        y_pred_probs.extend(batch_pred)  # Store full probability distribution\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    \n",
    "    num_classes = y_pred_probs.shape[1]\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "    # Set threshold search range\n",
    "    if threshold_range is None:\n",
    "        threshold_range = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "    # Find best threshold for each class based on F1-score\n",
    "    best_thresholds = []\n",
    "    best_f1_scores = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        best_threshold = 0.5  # Default\n",
    "        best_f1 = 0.0\n",
    "\n",
    "        for threshold in threshold_range:\n",
    "            temp_thresholds = [0.5] * num_classes  # Start with default 0.5 for all\n",
    "            temp_thresholds[i] = threshold  # Vary only the current class\n",
    "            \n",
    "            _, _, f1_scores, _ = calculate_metrics_multiclass(y_true, y_pred_probs, temp_thresholds)\n",
    "            if f1_scores[i] > best_f1:\n",
    "                best_f1 = f1_scores[i]\n",
    "                best_threshold = threshold\n",
    "\n",
    "        best_thresholds.append(best_threshold)\n",
    "        best_f1_scores.append(best_f1)\n",
    "\n",
    "    # Apply optimal thresholds\n",
    "    y_pred, cm, f1_scores, overall_f1 = calculate_metrics_multiclass(y_true, y_pred_probs, best_thresholds)\n",
    "\n",
    "    # Compute ROC Curve & AUC per class\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        y_true_binary = (y_true == i).astype(int)\n",
    "        y_pred_binary = y_pred_probs[:, i]\n",
    "\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_binary, y_pred_binary)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    macro_auc = np.mean(list(roc_auc.values()))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC Curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Multiclass ROC Curve (Macro AUC = {macro_auc:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    # Print optimal thresholds\n",
    "    print(\"\\nOptimal Per-Class Thresholds:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name}: {best_thresholds[i]:.2f} (F1 = {best_f1_scores[i]:.4f})\")\n",
    "\n",
    "    print(f\"\\nOverall Macro F1 Score: {overall_f1:.4f}\")\n",
    "    print(f\"Macro-Averaged ROC AUC: {macro_auc:.4f}\")\n",
    "\n",
    "    return best_thresholds\n",
    "\n",
    "\n",
    "# Usage:\n",
    "def evaluate_multiclass_with_adaptive_threshold(model, test_generator):\n",
    "    \"\"\"\n",
    "    Evaluate model on a multi-class dataset using adaptive thresholding.\n",
    "    \"\"\"\n",
    "    optimal_thresholds = adaptive_threshold_determination_multiclass2(model, test_generator)\n",
    "    return optimal_thresholds\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9d692-e475-4c26-a0b6-8c9195b4bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# %%\n",
    "cnn = tf.keras.models.load_model(\"/mnt/c/modelFiles/ensemble_greyColor_quantized_average_model.keras\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe5551-0ecd-432d-9e2e-71e37b2b2c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''try:\n",
    "    evaluate_multiclass_with_adaptive_threshold(cnn,test_generator)\n",
    "except StopIteration as e:\n",
    "    print(\"iteration stopped. evaluating again.\")\n",
    "    evaluate_multiclass_with_adaptive_threshold(cnn,test_generator)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46fba5-744e-4d80-b2ef-0abcf51e7ccf",
   "metadata": {},
   "source": [
    "***QAT GENERIC KERAS MODEL RESOURCE UTILISATIONS AND INFERENCE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa349c82-95d1-44cf-aa00-f9834e868e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# %%\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "# !pip install gputil\n",
    "# !pip install pynvml\n",
    "\n",
    "import GPUtil'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73343525-ca59-4620-9fff-30f4fdadbe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Load the trained model\n",
    "\n",
    "model = tf.keras.models.load_model(\"/mnt/c/modelFiles/ensemble_greyColor_quantized_average_model.keras\")\n",
    "\n",
    "# Load a single test image (modify path as needed)\n",
    "img_path = \"/mnt/c/newTrain/Train/line/17.jpg\"\n",
    "# Load as grayscale\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (256, 256))\n",
    "img = img.astype(np.float32) / 255.0  # Normalize\n",
    "\n",
    "# Compute Sobel edge detection\n",
    "sobelx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)\n",
    "sobely = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=3)\n",
    "\n",
    "# Normalize edges to 0-1\n",
    "sobelx = cv2.normalize(sobelx, None, 0, 1, cv2.NORM_MINMAX)\n",
    "sobely = cv2.normalize(sobely, None, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "# Stack channels (grayscale repeated 3 times + Sobel X + Sobel Y)\n",
    "stacked = np.stack([img], axis=-1)\n",
    "\n",
    "# Add batch dimension\n",
    "img_array = np.expand_dims(stacked, axis=0)  # Shape becomes (1, 256, 256, 3)\n",
    "\n",
    "# Measure cold start time (time to first prediction)\n",
    "t1 = time.time()\n",
    "prediction = model.predict(img_array)\n",
    "t2 = time.time()\n",
    "cold_start_time = t2 - t1\n",
    "\n",
    "# Measure inference time for a single image\n",
    "num_trials = 10\n",
    "times = []\n",
    "for _ in range(num_trials):\n",
    "    t1 = time.time()\n",
    "    _ = model.predict(img_array)\n",
    "    t2 = time.time()\n",
    "    times.append(t2 - t1)\n",
    "\n",
    "inference_time = np.mean(times)\n",
    "\n",
    "# Model size\n",
    "model_size = os.path.getsize(\"/mnt/c/modelFiles/ensemble_greyColor_quantized_average_model.keras\") / (1024 * 1024)  # In MB\n",
    "\n",
    "# Memory usage before and after prediction\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss / (1024 * 1024)  # In MB\n",
    "model.predict(img_array)\n",
    "mem_after = process.memory_info().rss / (1024 * 1024)  # In MB\n",
    "memory_usage = mem_after - mem_before\n",
    "\n",
    "# GPU utilization\n",
    "gpus = GPUtil.getGPUs()\n",
    "gpu_usage = gpus[0].load * 100 if gpus else None\n",
    "\n",
    "# CPU utilization\n",
    "cpu_util = psutil.cpu_percent(interval=1)\n",
    "\n",
    "# Power consumption (only works on supported systems)\n",
    "power_usage = None\n",
    "try:\n",
    "    power_usage = psutil.sensors_battery().power_plugged  # Approximate if available\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from tensorflow.compat.v1.profiler import profile\n",
    "from tensorflow.compat.v1.profiler import ProfileOptionBuilder\n",
    "\n",
    "def get_flops(model, input_shape):\n",
    "    concrete = tf.function(lambda inputs: model(inputs)).get_concrete_function(\n",
    "        tf.TensorSpec([1] + list(input_shape), model.dtype)\n",
    "    )\n",
    "    frozen_func = convert_variables_to_constants_v2(concrete)\n",
    "    graph = frozen_func.graph\n",
    "\n",
    "    # Use TensorFlow's profiler\n",
    "    run_meta = tf.compat.v1.RunMetadata()\n",
    "    opts = ProfileOptionBuilder.float_operation()\n",
    "    \n",
    "    flops = profile(graph, run_meta=run_meta, options=opts)\n",
    "    \n",
    "    return flops.total_float_ops  # Total FLOPs\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (256, 256, 1)  # Adjust based on your model\n",
    "flops = get_flops(model, input_shape)\n",
    "print(\"FLOPs:\", flops)\n",
    "\n",
    "\n",
    "# Number of parameters\n",
    "num_params = model.count_params()\n",
    "\n",
    "# Data loading time\n",
    "t1 = time.time()\n",
    "_ = tf.keras.preprocessing.image.load_img(img_path, target_size=(256, 256))\n",
    "t2 = time.time()\n",
    "data_loading_time = t2 - t1\n",
    "\n",
    "# Print results\n",
    "print(f\"Cold Start Time: {cold_start_time:.4f} seconds\")\n",
    "print(f\"Average Inference Time: {inference_time:.4f} seconds\")\n",
    "print(f\"Model Size: {model_size:.2f} MB\")\n",
    "print(f\"Memory Usage: {memory_usage:.2f} MB\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Number of Parameters: {num_params}\")\n",
    "print(f\"GPU Utilization: {gpu_usage:.2f}%\" if gpu_usage is not None else \"GPU Utilization: Not Available\")\n",
    "print(f\"CPU Utilization: {cpu_util:.2f}%\")\n",
    "print(f\"Power Consumption w/o GPU: {power_usage}\")\n",
    "print(f\"Data Loading Time: {data_loading_time:.4f} seconds\")\n",
    "import pynvml\n",
    "\n",
    "def get_gpu_power():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # GPU 0\n",
    "    power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000  # Convert to Watts\n",
    "    pynvml.nvmlShutdown()\n",
    "    return power  # Power in Watts\n",
    "\n",
    "power_watts = get_gpu_power()\n",
    "print(\"power watts (GPU):\",power_watts)\n",
    "\n",
    "def get_sparsity_ratio(model):\n",
    "    total_params = np.sum([np.prod(w.shape) for w in model.weights])\n",
    "    zero_params = np.sum([np.sum(w.numpy() == 0) for w in model.weights])\n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity\n",
    "\n",
    "def get_energy_efficiency(flops, power_watts=None):\n",
    "    if power_watts is None:\n",
    "        return \"Power consumption data missing\"\n",
    "    return flops / power_watts  # FLOPs per Watt\n",
    "\n",
    "# Example usage:\n",
    "sparsity_ratio = get_sparsity_ratio(model)\n",
    "print(\"Sparsity Ratio:\", sparsity_ratio)\n",
    "\n",
    "# Replace 'power_watts' with the actual power usage if available\n",
    "energy_efficiency = get_energy_efficiency(flops, power_watts)\n",
    "print(\"Energy Efficiency:\", energy_efficiency)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa190fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import cv2\n",
    "import GPUtil\n",
    "import pynvml\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from tensorflow.compat.v1.profiler import profile\n",
    "from tensorflow.compat.v1.profiler import ProfileOptionBuilder\n",
    "\n",
    "model_paths = [\n",
    "    \"/mnt/c/modelFiles/greyColor_quantized_kfold_model_0.keras\",\n",
    "    \"/mnt/c/modelFiles/greyColor_quantized_kfold_model_1.keras\",\n",
    "    \"/mnt/c/modelFiles/greyColor_quantized_kfold_model_2.keras\",\n",
    "    \"/mnt/c/modelFiles/greyColor_quantized_kfold_model_3.keras\",\n",
    "    \"/mnt/c/modelFiles/greyColor_quantized_kfold_model_4.keras\"\n",
    "]\n",
    "\n",
    "# Load a single test image (modify path as needed)\n",
    "img_path = \"/mnt/c/newTrain/Train/line/17.jpg\"\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (256, 256))\n",
    "img = img.astype(np.float32) / 255.0\n",
    "\n",
    "# Stack into 1 channel\n",
    "stacked = np.stack([img], axis=-1)  # shape: (256, 256, 1)\n",
    "img_array = np.expand_dims(stacked, axis=0)  # shape: (1, 256, 256, 1)\n",
    "\n",
    "# Cold start timing\n",
    "t1 = time.time()\n",
    "ensemble_probs = np.mean([model.predict(img_array, verbose=0) for model in models], axis=0)\n",
    "t2 = time.time()\n",
    "cold_start_time = t2 - t1\n",
    "\n",
    "# Inference timing\n",
    "num_trials = 10\n",
    "times = []\n",
    "for _ in range(num_trials):\n",
    "    t1 = time.time()\n",
    "    _ = np.mean([model.predict(img_array, verbose=0) for model in models], axis=0)\n",
    "    t2 = time.time()\n",
    "    times.append(t2 - t1)\n",
    "inference_time = np.mean(times)\n",
    "\n",
    "# Average model size\n",
    "model_size = np.mean([os.path.getsize(path) for path in model_paths]) / (1024 * 1024)\n",
    "\n",
    "# Memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss / (1024 * 1024)\n",
    "_ = np.mean([model.predict(img_array, verbose=0) for model in models], axis=0)\n",
    "mem_after = process.memory_info().rss / (1024 * 1024)\n",
    "memory_usage = mem_after - mem_before\n",
    "\n",
    "# GPU and CPU stats\n",
    "gpus = GPUtil.getGPUs()\n",
    "gpu_usage = gpus[0].load * 100 if gpus else None\n",
    "cpu_util = psutil.cpu_percent(interval=1)\n",
    "\n",
    "# Power usage (CPU only)\n",
    "try:\n",
    "    power_usage = psutil.sensors_battery().power_plugged\n",
    "except AttributeError:\n",
    "    power_usage = None\n",
    "\n",
    "# GPU power usage\n",
    "def get_gpu_power():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000\n",
    "    pynvml.nvmlShutdown()\n",
    "    return power\n",
    "power_watts = get_gpu_power()\n",
    "\n",
    "# FLOPs for one model (assuming same for all)\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "def get_flops(model, input_shape):\n",
    "    concrete = tf.function(lambda inputs: model(inputs)).get_concrete_function(\n",
    "        tf.TensorSpec([1] + list(input_shape), model.inputs[0].dtype)\n",
    "    )\n",
    "    frozen_func = convert_variables_to_constants_v2(concrete)\n",
    "    graph = frozen_func.graph\n",
    "\n",
    "    run_meta = tf.compat.v1.RunMetadata()\n",
    "    opts = ProfileOptionBuilder.float_operation()\n",
    "\n",
    "    # Suppress stdout\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        flops = profile(graph, run_meta=run_meta, options=opts)\n",
    "    \n",
    "    return flops.total_float_ops\n",
    "\n",
    "input_shape = (256, 256, 1)\n",
    "flops = get_flops(models[0], input_shape) * len(models)  # total ensemble FLOPs\n",
    "\n",
    "# Parameter count\n",
    "num_params = sum([model.count_params() for model in models])\n",
    "\n",
    "# Data load timing\n",
    "t1 = time.time()\n",
    "_ = tf.keras.preprocessing.image.load_img(img_path, target_size=(256, 256))\n",
    "t2 = time.time()\n",
    "data_loading_time = t2 - t1\n",
    "\n",
    "# Sparsity\n",
    "def get_sparsity_ratio(model):\n",
    "    total_params = np.sum([np.prod(w.shape) for w in model.weights])\n",
    "    zero_params = np.sum([np.sum(w.numpy() == 0) for w in model.weights])\n",
    "    return zero_params / total_params\n",
    "\n",
    "sparsity_ratios = [get_sparsity_ratio(model) for model in models]\n",
    "avg_sparsity = np.mean(sparsity_ratios)\n",
    "\n",
    "# Energy efficiency\n",
    "def get_energy_efficiency(flops, power_watts=None):\n",
    "    if power_watts is None:\n",
    "        return \"Power consumption data missing\"\n",
    "    return flops / power_watts\n",
    "\n",
    "energy_efficiency = get_energy_efficiency(flops, power_watts)\n",
    "\n",
    "# Print results\n",
    "print(f\"Cold Start Time: {cold_start_time:.4f} seconds\")\n",
    "print(f\"Average Inference Time: {inference_time:.4f} seconds\")\n",
    "print(f\"Average Model Size: {model_size:.2f} MB\")\n",
    "print(f\"Memory Usage: {memory_usage:.2f} MB\")\n",
    "print(f\"Total FLOPs (Ensemble): {flops}\")\n",
    "print(f\"Total Parameters (Ensemble): {num_params}\")\n",
    "print(f\"GPU Utilization: {gpu_usage:.2f}%\" if gpu_usage is not None else \"GPU Utilization: Not Available\")\n",
    "print(f\"CPU Utilization: {cpu_util:.2f}%\")\n",
    "print(f\"Power Consumption (CPU): {power_usage}\")\n",
    "print(f\"Power Watts (GPU): {power_watts}\")\n",
    "print(f\"Data Loading Time: {data_loading_time:.4f} seconds\")\n",
    "print(f\"Average Sparsity Ratio: {avg_sparsity:.4f}\")\n",
    "print(f\"Energy Efficiency (FLOPs/Watt): {energy_efficiency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a9f78-2d4a-4dc2-b558-eca9249411f3",
   "metadata": {},
   "source": [
    "***QUANTIZED TFLITE MODEL RESOURCE UTILISATIONS AND INFERENCE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93734d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import GPUtil\n",
    "import pynvml\n",
    "import tensorflow as tf\n",
    "\n",
    "# Paths to your 5 TFLite models\n",
    "model_paths = [\n",
    "    \"/mnt/c/modelFiles/quantized_kfold_model_0.tflite\",\n",
    "    \"/mnt/c/modelFiles/quantized_kfold_model_1.tflite\",\n",
    "    \"/mnt/c/modelFiles/quantized_kfold_model_2.tflite\",\n",
    "    \"/mnt/c/modelFiles/quantized_kfold_model_3.tflite\",\n",
    "    \"/mnt/c/modelFiles/quantized_kfold_model_4.tflite\"\n",
    "]\n",
    "\n",
    "# Load interpreters\n",
    "interpreters = [tf.lite.Interpreter(model_path=path) for path in model_paths]\n",
    "for interpreter in interpreters:\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreters[0].get_input_details()\n",
    "output_details = interpreters[0].get_output_details()\n",
    "\n",
    "# Preprocess image\n",
    "img_path = \"/mnt/c/newTrain/Train/line/17.jpg\"\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (256, 256))\n",
    "img = img.astype(np.float32) / 255.0\n",
    "\n",
    "# Sobel edges\n",
    "sobelx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)\n",
    "sobely = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=3)\n",
    "sobelx = cv2.normalize(sobelx, None, 0, 1, cv2.NORM_MINMAX)\n",
    "sobely = cv2.normalize(sobely, None, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "# Stack channels\n",
    "stacked = np.stack([img], axis=-1)\n",
    "\n",
    "# Quantize input\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "img_input = stacked / input_scale + input_zero_point\n",
    "img_input = np.clip(img_input, -128, 127).astype(np.int8)\n",
    "img_input = np.expand_dims(img_input, axis=0)\n",
    "\n",
    "# Cold start time (for one model)\n",
    "t1 = time.time()\n",
    "interpreters[0].set_tensor(input_details[0]['index'], img_input)\n",
    "interpreters[0].invoke()\n",
    "_ = interpreters[0].get_tensor(output_details[0]['index'])\n",
    "t2 = time.time()\n",
    "cold_start_time = t2 - t1\n",
    "\n",
    "# Inference time over 10 trials (ensemble)\n",
    "times = []\n",
    "for _ in range(10):\n",
    "    t1 = time.time()\n",
    "    predictions = []\n",
    "    for interpreter in interpreters:\n",
    "        interpreter.set_tensor(input_details[0]['index'], img_input)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions.append(output_data[0])\n",
    "    _ = np.mean(predictions, axis=0)\n",
    "    t2 = time.time()\n",
    "    times.append(t2 - t1)\n",
    "inference_time = np.mean(times)\n",
    "\n",
    "# Model size (sum of all 5)\n",
    "model_size = sum(os.path.getsize(path) for path in model_paths) / (1024 * 1024)\n",
    "\n",
    "# Memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss / (1024 * 1024)\n",
    "for interpreter in interpreters:\n",
    "    interpreter.set_tensor(input_details[0]['index'], img_input)\n",
    "    interpreter.invoke()\n",
    "    _ = interpreter.get_tensor(output_details[0]['index'])\n",
    "mem_after = process.memory_info().rss / (1024 * 1024)\n",
    "memory_usage = mem_after - mem_before\n",
    "\n",
    "# GPU usage\n",
    "gpus = GPUtil.getGPUs()\n",
    "gpu_usage = gpus[0].load * 100 if gpus else None\n",
    "\n",
    "# CPU usage\n",
    "cpu_util = psutil.cpu_percent(interval=1)\n",
    "\n",
    "# Power (battery)\n",
    "try:\n",
    "    power_usage = psutil.sensors_battery().power_plugged\n",
    "except:\n",
    "    power_usage = None\n",
    "\n",
    "# GPU power\n",
    "def get_gpu_power():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000\n",
    "    pynvml.nvmlShutdown()\n",
    "    return power\n",
    "\n",
    "power_watts = get_gpu_power()\n",
    "\n",
    "# Data loading time\n",
    "t1 = time.time()\n",
    "_ = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "t2 = time.time()\n",
    "data_loading_time = t2 - t1\n",
    "\n",
    "# Print results\n",
    "print(f\"Cold Start Time: {cold_start_time:.4f} s\")\n",
    "print(f\"Average Inference Time (Ensemble): {inference_time:.4f} s\")\n",
    "print(f\"average Model Size (5 models): {model_size/5:.2f} MB\")\n",
    "print(f\"Memory Usage: {memory_usage:.2f} MB\")\n",
    "print(f\"GPU Utilization: {gpu_usage:.2f}%\" if gpu_usage else \"GPU Utilization: Not Available\")\n",
    "print(f\"CPU Utilization: {cpu_util:.2f}%\")\n",
    "print(f\"Power Plugged In: {power_usage}\")\n",
    "print(f\"GPU Power (W): {power_watts}\")\n",
    "print(f\"Data Loading Time: {data_loading_time:.4f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d2a2b-1cfd-4107-96d6-06651c590000",
   "metadata": {},
   "source": [
    "**ENDING CODES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6de16-1956-46bc-8b32-bbb77dc8c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "!pip freeze > requirements.txt\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
